# Quick Start Guide

**Get started with RL Dispatch MVP in 5 minutes!**

---

## Installation

### 1. Clone or navigate to repository

```bash
cd rl_dispatch_mvp
```

### 2. Install package

```bash
# Install in development mode
pip install -e .

# Or install with development dependencies
pip install -e ".[dev]"
```

### 3. Verify installation

```bash
python -c "from rl_dispatch.env import PatrolEnv; print('âœ… Installation successful!')"
```

---

## Quick Test Run

### Test the environment

```python
from rl_dispatch.env import PatrolEnv

# Create environment
env = PatrolEnv()

# Reset environment
obs, info = env.reset()
print(f"Observation shape: {obs.shape}")  # (77,)

# Take a random action
action = env.action_space.sample()
obs, reward, terminated, truncated, info = env.step(action)
print(f"Reward: {reward:.2f}")
print(f"Info: {info}")
```

### Test candidate generation

```python
from rl_dispatch.planning import CandidateFactory
from rl_dispatch.core.types import RobotState, PatrolPoint

# Create sample data
robot = RobotState(x=5.0, y=5.0, heading=0.0, velocity=0.5,
                  angular_velocity=0.0, battery_level=0.8)
patrol_points = (
    PatrolPoint(x=10.0, y=10.0, last_visit_time=0.0, priority=1.0, point_id=0),
    PatrolPoint(x=40.0, y=10.0, last_visit_time=50.0, priority=1.0, point_id=1),
)

# Generate all candidates
factory = CandidateFactory()
candidates = factory.generate_all(robot, patrol_points, current_time=100.0)

# Print strategies
for candidate in candidates:
    print(f"{candidate.strategy_name}: {candidate.patrol_order}")
```

---

## Training

### Option A: Single Map Training (Basic)

#### 1. Quick training (1M steps)

```bash
python scripts/train.py --seed 42 --cuda
```

#### 2. Custom configuration

```bash
python scripts/train.py \
    --env-config configs/custom_env.yaml \
    --reward-config configs/custom_reward.yaml \
    --run-name my_experiment \
    --seed 123
```

---

### Option B: Multi-Map Training (â­ Recommended for Generalization)

**ì¼ë°˜í™”ëœ ì •ì±… í•™ìŠµ - ë‹¤ì–‘í•œ ë§µì—ì„œ í›ˆë ¨í•˜ì—¬ ì–´ë–¤ í™˜ê²½ì—ì„œë„ ì˜ ì‘ë™í•˜ëŠ” ëª¨ë¸**

#### 1. ê¸°ë³¸ ë©€í‹°ë§µ í•™ìŠµ (6ê°œ ë‹¤ì–‘í•œ ë§µ)

```bash
python scripts/train_multi_map.py \
    --total-timesteps 5000000 \
    --seed 42 \
    --cuda
```

ìë™ìœ¼ë¡œ ë‹¤ìŒ ë§µë“¤ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:
- `map_large_square` - í° ì •ì‚¬ê°í˜• (100Ã—100m)
- `map_corridor` - ë³µë„í˜• (120Ã—30m)
- `map_l_shaped` - Lìí˜• ê±´ë¬¼ (80Ã—80m)
- `map_office_building` - ì‚¬ë¬´ì‹¤ (90Ã—70m)
- `map_campus` - ìº í¼ìŠ¤ (150Ã—120m)
- `map_warehouse` - ì°½ê³  (140Ã—100m)

#### 2. íŠ¹ì • ë§µë“¤ë§Œ ì„ íƒ

```bash
python scripts/train_multi_map.py \
    --maps configs/map_large_square.yaml configs/map_office_building.yaml \
    --map-mode random \
    --seed 42
```

#### 3. ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ (ì‰¬ìš´ ë§µ â†’ ì–´ë ¤ìš´ ë§µ)

```bash
python scripts/train_multi_map.py \
    --map-mode curriculum \
    --total-timesteps 10000000 \
    --seed 42 \
    --cuda
```

ì´ˆë°˜ì—ëŠ” ì‘ê³  ê°„ë‹¨í•œ ë§µì—ì„œ í•™ìŠµí•˜ê³ , ì ì°¨ í¬ê³  ë³µì¡í•œ ë§µìœ¼ë¡œ ì „í™˜ë©ë‹ˆë‹¤.

---

### 3. Monitor with TensorBoard

```bash
tensorboard --logdir runs
```

Open browser to http://localhost:6006

**ë©€í‹°ë§µ í•™ìŠµ ì‹œ TensorBoardì—ì„œ í™•ì¸ ê°€ëŠ¥:**
- ê° ë§µë³„ ì„±ëŠ¥ (episode_per_map/)
- ì „ì²´ í‰ê·  ì„±ëŠ¥ (episode/)
- ë§µë³„ ì´ë²¤íŠ¸ ì„±ê³µë¥ , ìˆœì°° ì»¤ë²„ë¦¬ì§€

---

## Evaluation

### Option A: Single Map Evaluation

```bash
python scripts/evaluate.py \
    --model checkpoints/my_experiment/final_model.pth \
    --episodes 100
```

---

### Option B: Multi-Map Generalization Evaluation (â­ Recommended)

**ì—¬ëŸ¬ ë§µì—ì„œ ì •ì±…ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.**

#### 1. ì¼ë°˜í™” ì„±ëŠ¥ í‰ê°€

```bash
python scripts/evaluate_generalization.py \
    --model checkpoints/multi_map_ppo/final.pth \
    --episodes 50 \
    --save-json
```

**ê²°ê³¼:**
- `outputs/generalization/generalization_results.json` - ìƒì„¸ ë°ì´í„°
- `outputs/generalization/generalization_comparison.png` - ë§µë³„ ì„±ëŠ¥ ë¹„êµ
- `outputs/generalization/return_distribution.png` - ë¦¬í„´ ë¶„í¬

**ì¶œë ¥ ì˜ˆì‹œ:**
```
Map                       Return               Event Success        Patrol Coverage
---------------------------------------------------------------------------------
map_large_square         523.4 Â± 45.2         87.3% Â± 8.1%        94.2% Â± 3.5%
map_office_building      -234.1 Â± 67.8        72.1% Â± 11.2%       88.6% Â± 5.2%
map_campus               312.5 Â± 89.3         81.4% Â± 9.7%        91.3% Â± 4.8%
```

#### 2. ì»¤ë²„ë¦¬ì§€ íˆíŠ¸ë§µ ì‹œê°í™”

```bash
python scripts/visualize_coverage.py \
    --model checkpoints/multi_map_ppo/final.pth \
    --episodes-per-map 20 \
    --output outputs/coverage_heatmaps.png
```

**í™•ì¸ ê°€ëŠ¥:**
- ê° ë§µì—ì„œ ë¡œë´‡ì´ ë°©ë¬¸í•œ ì˜ì—­ (íˆíŠ¸ë§µ)
- ìˆœì°° ì»¤ë²„ë¦¬ì§€ í†µê³„
- ì‚¬ê°ì§€ëŒ€ íŒŒì•…

---

### Quick Code Evaluation

```python
from rl_dispatch.env import PatrolEnv
from rl_dispatch.algorithms import PPOAgent

# Load trained agent
agent = PPOAgent()
agent.load("checkpoints/my_experiment/final_model.pth")

# Evaluate
env = PatrolEnv()
obs, _ = env.reset()

for _ in range(100):
    action, _, _ = agent.get_action(obs, deterministic=True)
    obs, reward, done, truncated, info = env.step(action)
    if done or truncated:
        print(f"Episode return: {info['episode']['r']:.2f}")
        obs, _ = env.reset()
```

---

## Baseline Comparison

### Test baseline policies

```python
from rl_dispatch.env import PatrolEnv
from rl_dispatch.algorithms.baselines import (
    B0_AlwaysPatrol,
    B1_AlwaysDispatch,
    B2_ThresholdDispatch,
    B3_UrgencyBased,
    B4_HeuristicPolicy,
)

env = PatrolEnv()
policy = B2_ThresholdDispatch()

# Run episode
obs, _ = env.reset()
done = False
episode_return = 0

while not done:
    state = env.current_state  # Get full state
    action = policy.select_action(state)
    action_array = [action.mode, action.replan_idx]
    obs, reward, done, truncated, info = env.step(action_array)
    episode_return += reward
    done = done or truncated

print(f"Baseline return: {episode_return:.2f}")
```

---

## Run Tests

```bash
# Install pytest
pip install pytest pytest-cov

# Run all tests
pytest tests/

# Run with coverage
pytest tests/ --cov=src --cov-report=html

# Run specific test file
pytest tests/test_env.py -v
```

---

## Project Structure Overview

```
rl_dispatch_mvp/
â”œâ”€â”€ src/rl_dispatch/        # Main package
â”‚   â”œâ”€â”€ core/               # Data structures & configs
â”‚   â”œâ”€â”€ env/                # Gymnasium environment
â”‚   â”œâ”€â”€ algorithms/         # PPO & baselines
â”‚   â”œâ”€â”€ planning/           # Candidate generation
â”‚   â”œâ”€â”€ rewards/            # Reward calculation
â”‚   â””â”€â”€ utils/              # Utilities
â”‚
â”œâ”€â”€ scripts/                # Training & evaluation
â”‚   â”œâ”€â”€ train.py           # Main training script
â”‚   â””â”€â”€ evaluate.py        # Evaluation script
â”‚
â”œâ”€â”€ configs/                # Configuration files
â”‚   â””â”€â”€ default.yaml       # Default config
â”‚
â”œâ”€â”€ tests/                  # Unit tests
â”‚   â”œâ”€â”€ test_core_types.py
â”‚   â”œâ”€â”€ test_candidate_generation.py
â”‚   â””â”€â”€ test_env.py
â”‚
â””â”€â”€ readme/                 # Documentation
    â”œâ”€â”€ PROGRESS.md        # Development progress
    â”œâ”€â”€ development_guide.md
    â””â”€â”€ R&D_Plan_Complete_v4.md
```

---

## Common Tasks

### 1. Create custom environment config

```yaml
# configs/my_env.yaml
env:
  map_width: 100.0
  map_height: 100.0
  patrol_points:
    - [20.0, 20.0]
    - [80.0, 20.0]
    - [80.0, 80.0]
    - [20.0, 80.0]
  max_episode_steps: 300
  event_generation_rate: 8.0
```

### 2. Tune reward weights

```yaml
# configs/my_reward.yaml
reward:
  w_event: 1.5      # Increase event importance
  w_patrol: 0.8     # Increase patrol importance
  w_safety: 3.0     # Heavily penalize safety violations
  w_efficiency: 0.05  # Reduce efficiency penalty
```

### 3. Adjust training hyperparameters

```yaml
# configs/my_training.yaml
training:
  learning_rate: 0.0001  # Lower LR for stability
  total_timesteps: 20000000  # Longer training
  num_steps: 4096    # Larger batch
  num_epochs: 15     # More epochs per update
```

---

## Troubleshooting

### Issue: Import errors

```bash
# Ensure package is installed
pip install -e .

# Check installation
python -c "import rl_dispatch; print(rl_dispatch.__version__)"
```

### Issue: CUDA out of memory

```bash
# Use CPU instead
python scripts/train.py  # --cuda flag removed

# Or reduce batch size in training config
```

### Issue: Environment not converging

```bash
# Try different random seed
python scripts/train.py --seed 123

# Adjust reward weights
# Edit configs/default.yaml

# Increase learning rate
# Edit training config: learning_rate: 0.001
```

---

## Next Steps

1. **Read full documentation**: See `readme/` folder
2. **Experiment with configurations**: Modify `configs/default.yaml`
3. **Run ablation studies**: Test different reward weights
4. **Compare with baselines**: Use baseline policies
5. **Visualize results**: Use TensorBoard

---

## Getting Help

- **Documentation**: See `readme/` folder
- **Examples**: See `scripts/` folder
- **Tests**: See `tests/` folder for usage examples
- **Issues**: Open GitHub issue

---

**Happy Training! ğŸš€**
