# Policy Collapse ë¬¸ì œ ì§„ë‹¨ ë° í•´ê²° ê°€ì´ë“œ

**ì‘ì„±ì**: Reviewer ë°•ìš©ì¤€
**ì‘ì„±ì¼**: 2025-12-30
**ë¬¸ì œ**: Global step 245kâ†’491k ë™ì•ˆ ì„±ëŠ¥ ì•…í™” ë° ì •ì±… ë¶•ê´´

---

## ğŸ”¥ ê´€ì¸¡ëœ ì¦ìƒ

### 1. ì„±ëŠ¥ ì§€í‘œ ì•…í™”
- **Success Rate**: 55-59% â†’ 29-36% (ì§€ì† í•˜ë½)
- **Return**: ì ì  ë” ìŒìˆ˜ë¡œ ì•…í™”
- **Coverage**: 10-25% ì •ì²´ (ë‚®ìŒ)

### 2. í•™ìŠµ ì§€í‘œ ì´ìƒ
- **Entropy**: 0.0011 â†’ 0.0000 (íƒìƒ‰ ë¶•ê´´)
- **Approx KL**: â‰ˆ0 (ì •ì±… ì—…ë°ì´íŠ¸ ë©ˆì¶¤)
- **Clipfrac**: 0 (PPO clipping ì‘ë™ ì•ˆ í•¨)
- **Policy Loss**: â‰ˆ0 (ì—…ë°ì´íŠ¸ íš¨ê³¼ ì—†ìŒ)

### 3. Critic ë¶•ê´´
- **Value Loss**: 2.5M~3.1M (ë§¤ìš° ë†’ìŒ!)
- **Explained Variance**: 0.005~0.01 (targetì„ ì „í˜€ ì„¤ëª… ëª»í•¨)

---

## ğŸ” ê·¼ë³¸ ì›ì¸ ë¶„ì„

### âœ… ë°œê²¬ëœ ë²„ê·¸ (Critical)

#### 1. **Reward Normalization ë¯¸êµ¬í˜„** â­â­â­
**ìœ„ì¹˜**: `src/rl_dispatch/core/config.py:355`, `scripts/train_multi_map.py`

**ë¬¸ì œ**:
```python
# config.pyì— ì„¤ì •ë§Œ ìˆìŒ
normalize_rewards: bool = True
clip_rewards: float = 10.0

# ì‹¤ì œ train_multi_map.pyì—ì„œ ì‚¬ìš© ì•ˆ í•¨!
agent.buffer.add(
    reward=reward,  # Raw rewardë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    ...
)
```

**ì˜í–¥**:
- Raw reward scaleì´ ë§¤ìš° í¼ (-200 ~ +100 ë²”ìœ„)
- Value functionì´ huge target í•™ìŠµ ì‹œë„ â†’ value_loss í­ì£¼ (2.5M+)
- Advantage normalizationë„ ë¶ˆì•ˆì •
- Criticì´ í­ì£¼í•˜ë©´ policy gradientë„ ì—‰ë§ë¨

**í•´ê²°**:
```python
# Welford's online algorithmìœ¼ë¡œ running mean/std ê³„ì‚°
reward_normalizer = RunningMeanStd()
reward_normalizer.update([reward])
normalized_reward = (reward - mean) / sqrt(var + eps)
normalized_reward = np.clip(normalized_reward, -5.0, 5.0)
```

---

#### 2. **Entropy Coefficient ë„ˆë¬´ ë‚®ìŒ** â­â­
**ìœ„ì¹˜**: `src/rl_dispatch/core/config.py:348`

**ë¬¸ì œ**:
```python
entropy_coef: float = 0.01  # Too low!
```

**ì˜í–¥**:
- ì´ˆë°˜ë¶€í„° íƒìƒ‰ì´ ë¶€ì¡±
- ì •ì±…ì´ ë¹ ë¥´ê²Œ deterministicìœ¼ë¡œ ìˆ˜ë ´ (entropy â†’ 0)
- Local optimaì— ê°‡í˜ (ê³„ì† patrolë§Œ or dispatchë§Œ)
- Approx KL = 0 â†’ ì •ì±… ì—…ë°ì´íŠ¸ê°€ ë©ˆì¶¤

**í•´ê²°**:
```python
# ì´ˆë°˜: 0.1 (íƒìƒ‰ ê°•í™”)
# í›„ë°˜: 0.01 (exploitation)
# Annealing ì ìš©
progress = update / num_updates
entropy_coef = 0.1 * (1 - progress) + 0.01 * progress
```

---

#### 3. **Learning Rate ë„ˆë¬´ ë†’ìŒ** â­
**ìœ„ì¹˜**: `scripts/train_multi_map.py:230`

**ë¬¸ì œ**:
```python
default=3e-4  # ì´ˆê¸°ê°’ì´ ë†’ìŒ
```

**ì˜í–¥**:
- ë¶ˆì•ˆì •í•œ í•™ìŠµ (value loss ì§„ë™)
- Policy collapse ê°€ì†í™”
- Entropyê°€ ê¸‰ê²©íˆ 0ìœ¼ë¡œ ìˆ˜ë ´

**í•´ê²°**:
```python
default=1e-4  # 3e-4 â†’ 1e-4 (3ë°° ê°ì†Œ)
# ë˜ëŠ” LR annealing ê°•í™”
```

---

#### 4. **ì§„ë‹¨ ë¡œê¹… ë¶€ì¡±** â­
**ìœ„ì¹˜**: `scripts/train_multi_map.py` (ì „ë°˜)

**ë¬¸ì œ**:
- Valid action count ë¡œê·¸ ì—†ìŒ â†’ action mask ë¬¸ì œ ê°ì§€ ë¶ˆê°€
- Advantage/return stats ì—†ìŒ â†’ ë³´ìƒ ìŠ¤ì¼€ì¼ ë¬¸ì œ ê°ì§€ ë¶ˆê°€
- Reward normalization stats ì—†ìŒ â†’ normalize ì‘ë™ í™•ì¸ ë¶ˆê°€
- Action mode distribution ì—†ìŒ â†’ "ì¶œë™ë§Œ" or "ìˆœì°°ë§Œ" ê°ì§€ ë¶ˆê°€

**í•´ê²°**:
```python
# í•„ìˆ˜ ë¡œê·¸ ì¶”ê°€:
- diagnostics/valid_action_count_mean
- diagnostics/valid_action_count_min
- diagnostics/advantage_mean, advantage_std
- diagnostics/reward_mean_raw, reward_std_raw
- diagnostics/reward_mean_normalized
- diagnostics/action_patrol_ratio, action_dispatch_ratio
- diagnostics/value_return_gap
```

---

### âš ï¸ ì˜ì‹¬ë˜ëŠ” ë¬¸ì œ (ê²€ì¦ í•„ìš”)

#### 5. **Action Masking Shape ë¶ˆì¼ì¹˜ ê°€ëŠ¥**
**ìœ„ì¹˜**: `src/rl_dispatch/env/patrol_env.py:539`

**ì˜ì‹¬**:
```python
# patrol_env.py
def _compute_action_mask(self) -> np.ndarray:
    mask = np.ones((2, num_candidates), dtype=np.float32)
    # ... masking logic ...
    return mask  # Shape: (2, num_candidates)

# buffer.py
self.action_masks = np.ones((buffer_size, 20), dtype=np.float32)
```

**ë¬¸ì œ**:
- Envê°€ (2, K) shape ë°˜í™˜
- BufferëŠ” (20,) ê³ ì • í¬ê¸° ê¸°ëŒ€
- Flatten/reshape ë¡œì§ ë¶ˆì¼ì¹˜ ê°€ëŠ¥

**ê²€ì¦ ë°©ë²•**:
```python
# train loopì—ì„œ ë¡œê·¸ ì¶”ê°€
action_mask = info.get("action_mask")
print(f"Action mask shape: {action_mask.shape}")
print(f"Valid actions: {np.sum(action_mask > 0.5)}")
```

**ì„ì‹œ í•´ê²°** (ê²€ì¦ ì „):
```python
# patrol_env.pyì—ì„œ flatten
mask = mask.flatten()  # (2*num_candidates,)
return mask
```

---

#### 6. **Value Loss Coefficient ë„ˆë¬´ ë‚®ìŒ**
**ìœ„ì¹˜**: `src/rl_dispatch/core/config.py:349`

**í˜„ì¬**:
```python
value_loss_coef: float = 0.5
```

**ë¬¸ì œ**:
- Reward scaleì´ í¬ë©´ criticì´ ì œëŒ€ë¡œ í•™ìŠµ ëª»í•¨
- Value lossê°€ policy lossì— ë¹„í•´ ë„ˆë¬´ ì•½í•¨

**í•´ê²°**:
```python
value_loss_coef: float = 1.0  # 0.5 â†’ 1.0
```

---

## ğŸ› ï¸ ì ìš©ëœ ìˆ˜ì • ì‚¬í•­

### `train_multi_map_fixed.py`ì— êµ¬í˜„ëœ ê°œì„ ì‚¬í•­

#### 1. **Reward Normalization ì™„ì „ êµ¬í˜„** âœ…
```python
class RunningMeanStd:
    """Welford's online algorithm"""
    def __init__(self, epsilon=1e-4, shape=()):
        self.mean = np.zeros(shape, dtype=np.float64)
        self.var = np.ones(shape, dtype=np.float64)
        self.count = epsilon

    def update(self, x):
        # Online mean/var update
        ...

# Train loopì—ì„œ ì‚¬ìš©
reward_normalizer = RunningMeanStd()

for step in range(num_steps):
    next_obs, reward, done, trunc, info = env.step(action)

    # Normalize reward
    reward_normalizer.update(np.array([reward]))
    normalized_reward = (reward - reward_normalizer.mean) / np.sqrt(reward_normalizer.var + 1e-8)
    normalized_reward = np.clip(normalized_reward, -5.0, 5.0)

    # Store normalized reward
    agent.buffer.add(reward=normalized_reward, ...)
```

#### 2. **Entropy Annealing** âœ…
```python
# ì´ˆë°˜: íƒìƒ‰ ê°•í™” (0.1)
# í›„ë°˜: exploitation (0.01)
for update in range(num_updates):
    progress = update / num_updates
    current_entropy_coef = 0.1 * (1.0 - progress) + 0.01 * progress
    agent.training_config.entropy_coef = current_entropy_coef
```

#### 3. **ê°œì„ ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°** âœ…
```python
parser.add_argument("--learning-rate", type=float, default=1e-4)  # 3e-4 â†’ 1e-4
parser.add_argument("--entropy-coef", type=float, default=0.1)    # 0.01 â†’ 0.1 (annealed)
parser.add_argument("--value-loss-coef", type=float, default=1.0) # 0.5 â†’ 1.0
parser.add_argument("--save-interval", type=int, default=50)      # 100 â†’ 50
```

#### 4. **í¬ê´„ì ì¸ ì§„ë‹¨ ë¡œê¹…** âœ…
```python
# Reward statistics
writer.add_scalar("diagnostics/reward_mean_raw", np.mean(raw_rewards), global_step)
writer.add_scalar("diagnostics/reward_std_raw", np.std(raw_rewards), global_step)
writer.add_scalar("diagnostics/reward_mean_normalized", reward_normalizer.mean, global_step)

# Action distribution
writer.add_scalar("diagnostics/action_patrol_ratio", patrol_ratio, global_step)
writer.add_scalar("diagnostics/action_dispatch_ratio", dispatch_ratio, global_step)

# Valid actions
writer.add_scalar("diagnostics/valid_action_count_mean", np.mean(valid_actions), global_step)
writer.add_scalar("diagnostics/valid_action_count_min", np.min(valid_actions), global_step)

# Advantage/value/return
writer.add_scalar("diagnostics/advantage_mean", np.mean(advantages), global_step)
writer.add_scalar("diagnostics/advantage_std", np.std(advantages), global_step)
writer.add_scalar("diagnostics/value_mean", np.mean(values), global_step)
writer.add_scalar("diagnostics/return_mean", np.mean(returns), global_step)
writer.add_scalar("diagnostics/value_return_gap", np.mean(returns - values), global_step)

# Entropy coefficient (annealing)
writer.add_scalar("train/entropy_coef", current_entropy_coef, global_step)
```

---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### 1. ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (10K steps, ~1ë¶„)
```bash
python scripts/train_multi_map_fixed.py \
    --total-timesteps 10000 \
    --seed 42 \
    --log-interval 1

# í™•ì¸ ì‚¬í•­:
# - ì—ëŸ¬ ì—†ì´ ì‹¤í–‰ë¨
# - diagnostics/* ë¡œê·¸ ìƒì„±ë¨
# - entropyê°€ 0ìœ¼ë¡œ ë¶•ê´´í•˜ì§€ ì•ŠìŒ
# - value_lossê°€ ì´ˆê¸° ê°’ë³´ë‹¤ ê°ì†Œí•¨
```

### 2. ì§§ì€ í•™ìŠµ (100K steps, ~5-10ë¶„)
```bash
python scripts/train_multi_map_fixed.py \
    --total-timesteps 100000 \
    --seed 42 \
    --log-interval 5 \
    --save-interval 20

# TensorBoardë¡œ ëª¨ë‹ˆí„°ë§
tensorboard --logdir runs/multi_map_ppo_fixed
```

### 3. ì „ì²´ í•™ìŠµ (5M steps, ~1-3ì‹œê°„)
```bash
python scripts/train_multi_map_fixed.py \
    --total-timesteps 5000000 \
    --seed 42 \
    --cuda \
    --log-interval 10 \
    --save-interval 50
```

---

## ğŸ“Š ê¸°ëŒ€ ê²°ê³¼

### ì´ˆê¸° (0-100K steps)
- âœ… Entropy: **0.08-0.10** ìœ ì§€ (0ìœ¼ë¡œ ë¶•ê´´ ì•ˆ í•¨)
- âœ… Approx KL: **0.005-0.02** (0ì´ ì•„ë‹˜, ì •ì±… ì—…ë°ì´íŠ¸ ì‘ë™)
- âœ… Clipfrac: **0.05-0.15** (PPO clipping ì‘ë™)
- âœ… Value Loss: **<1M** (2.5M+ì—ì„œ ê°ì†Œ)
- âœ… Explained Variance: **>0.1** (0.005ì—ì„œ ì¦ê°€)

### ì¤‘ê¸° (100K-500K steps)
- âœ… Success Rate: **ìƒìŠ¹ ë˜ëŠ” ì •ì²´** (í•˜ë½ ë©ˆì¶¤)
- âœ… Return: **ìµœì†Œ ì •ì²´** (ê³„ì† ì•…í™” ë©ˆì¶¤)
- âœ… Coverage: **ì ì§„ì  ì¦ê°€** (10% â†’ 30%+)
- âœ… Entropy: **0.06-0.08** (annealingìœ¼ë¡œ ì²œì²œíˆ ê°ì†Œ)

### í›„ê¸° (500K-5M steps)
- âœ… Success Rate: **60-75%+**
- âœ… Return: **ì–‘ìˆ˜ ë˜ëŠ” -1000 ì´ìƒ**
- âœ… Coverage: **50-80%+**
- âœ… Entropy: **0.02-0.04** (exploitationìœ¼ë¡œ ìˆ˜ë ´, 0ì€ ì•„ë‹˜)
- âœ… Explained Variance: **>0.5**

---

## ğŸ” TensorBoard ëª¨ë‹ˆí„°ë§ ê°€ì´ë“œ

### í•„ìˆ˜ í™•ì¸ ê·¸ë˜í”„

#### 1. **train/entropy** (ê°€ì¥ ì¤‘ìš”!)
```
ì •ìƒ: 0.08~0.10 â†’ 0.06~0.08 â†’ 0.02~0.04 (ì„œì„œíˆ ê°ì†Œ)
ë¹„ì •ìƒ: 0.01 â†’ 0.001 â†’ 0.0000 (ê¸‰ê²©íˆ 0ìœ¼ë¡œ ë¶•ê´´)
```

#### 2. **train/approx_kl**
```
ì •ìƒ: 0.005~0.02 (ì ë‹¹í•œ ë³€ë™)
ë¹„ì •ìƒ: 0.0000... (ì •ì±… ì—…ë°ì´íŠ¸ ë©ˆì¶¤)
```

#### 3. **train/clipfrac**
```
ì •ìƒ: 0.05~0.20 (PPO clipping ì‘ë™)
ë¹„ì •ìƒ: 0.000 (clipping ì•ˆ ì¼ì–´ë‚¨ = ì—…ë°ì´íŠ¸ ì—†ìŒ)
```

#### 4. **train/value_loss**
```
ì •ìƒ: ì´ˆê¸° ë†’ìŒ â†’ ì ì§„ì  ê°ì†Œ â†’ ì•ˆì •í™”
ë¹„ì •ìƒ: 2.5M+ ìœ ì§€ ë˜ëŠ” ê³„ì† ì¦ê°€
```

#### 5. **train/explained_variance**
```
ì •ìƒ: ì´ˆê¸° ë‚®ìŒ â†’ 0.5+ ì¦ê°€
ë¹„ì •ìƒ: 0.005~0.01 ì •ì²´ (criticì´ target ì„¤ëª… ëª»í•¨)
```

#### 6. **diagnostics/valid_action_count_mean**
```
ì •ìƒ: 10-20 (ì¶©ë¶„í•œ ì„ íƒì§€)
ë¹„ì •ìƒ: 1-2 (action maskê°€ ë„ˆë¬´ ì œí•œì )
```

#### 7. **diagnostics/action_patrol_ratio vs action_dispatch_ratio**
```
ì •ìƒ: ê· í˜•ì¡íŒ ë¶„í¬ (ì˜ˆ: 70% patrol, 30% dispatch)
ë¹„ì •ìƒ: í•œìª½ìœ¼ë¡œ ì¹˜ìš°ì¹¨ (ì˜ˆ: 99% patrol, 1% dispatch)
```

#### 8. **diagnostics/reward_mean_normalized**
```
ì •ìƒ: 0 ê·¼ì²˜ì—ì„œ ì•ˆì •í™” (normalization ì‘ë™)
ë¹„ì •ìƒ: í° ê°’ ìœ ì§€ (normalization ì•ˆ ë¨)
```

---

## ğŸ› ì¶”ê°€ ë””ë²„ê¹… íŒ

### 1. **Entropyê°€ ì—¬ì „íˆ 0ìœ¼ë¡œ ë¶•ê´´í•œë‹¤ë©´**
```python
# entropy_coefë¥¼ ë” ë†’ì´ê±°ë‚˜ annealing ì†ë„ ì¤„ì„
entropy_coef = 0.15 * (1 - progress**0.5) + 0.02 * progress**0.5
# ë˜ëŠ” ê³ ì •ê°’
entropy_coef = 0.05  # No annealing
```

### 2. **Value lossê°€ ì—¬ì „íˆ í¬ë‹¤ë©´**
```python
# Reward scale ë¬¸ì œì¼ ê°€ëŠ¥ì„±
# 1. clip_rewardsë¥¼ ë” ë‚®ì¶¤
clip_rewards = 3.0  # 5.0 â†’ 3.0

# 2. ë˜ëŠ” reward config ìì²´ë¥¼ ì¡°ì •
reward_config = RewardConfig(
    event_response_bonus=25.0,  # 50.0 â†’ 25.0
    collision_penalty=-50.0,     # -100.0 â†’ -50.0
    # ...
)
```

### 3. **Success rateê°€ ê³„ì† ë‚®ë‹¤ë©´**
```python
# Event reward weight ì¦ê°€
reward_config.w_event = 2.0  # 1.0 â†’ 2.0
reward_config.w_patrol = 0.3  # 0.5 â†’ 0.3
```

### 4. **Coverageê°€ ë‚®ë‹¤ë©´**
```python
# Patrol reward weight ì¦ê°€
reward_config.w_patrol = 1.0  # 0.5 â†’ 1.0
reward_config.w_event = 0.8   # 1.0 â†’ 0.8
```

---

## ğŸ“Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

í•™ìŠµ ì‹œì‘ ì „:
- [ ] `train_multi_map_fixed.py` ì‚¬ìš© (ê¸°ì¡´ ìŠ¤í¬ë¦½íŠ¸ ì•„ë‹˜!)
- [ ] `--log-interval 1` ë˜ëŠ” `10` ì„¤ì • (ìì£¼ ë¡œê¹…)
- [ ] TensorBoard ì‹¤í–‰: `tensorboard --logdir runs`

í•™ìŠµ ì¤‘ (ë§¤ 100K stepsë§ˆë‹¤):
- [ ] `train/entropy` > 0.02 í™•ì¸
- [ ] `train/approx_kl` > 0.001 í™•ì¸
- [ ] `train/clipfrac` > 0.02 í™•ì¸
- [ ] `train/value_loss` < 1M í™•ì¸
- [ ] `train/explained_variance` > 0.1 í™•ì¸
- [ ] `diagnostics/valid_action_count_mean` > 5 í™•ì¸
- [ ] `episode_per_map/*/return` í•˜ë½ ë©ˆì¶¤ í™•ì¸

---

## ğŸ¯ ì„±ê³µ ê¸°ì¤€

### Minimum Acceptable Performance (MAP)
- Entropy: **> 0.015** (ì „ êµ¬ê°„)
- Approx KL: **> 0.001** (ì—…ë°ì´íŠ¸ ì‘ë™)
- Value Loss: **< 1M** (ì•ˆì •í™”)
- Explained Variance: **> 0.2**
- Success Rate: **í•˜ë½ ë©ˆì¶¤ + ì •ì²´ or ìƒìŠ¹**

### Target Performance (TP)
- Entropy: **0.03-0.08** (annealing curve)
- Success Rate: **60%+**
- Coverage: **50%+**
- Return: **-3000 ì´ìƒ**

---

## ğŸ“ ë¬¸ì œ ë°œìƒ ì‹œ

1. **TensorBoard ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜** (íŠ¹íˆ entropy, approx_kl, value_loss)
2. **Console ì¶œë ¥ ë³µì‚¬** (Diagnostics ì„¹ì…˜)
3. **ì‹¤í–‰ ëª…ë ¹ì–´ ê¸°ë¡**
4. GitHub Issue ë˜ëŠ” íŒ€ì— ê³µìœ 

---

**ì‘ì„±ì**: Reviewer ë°•ìš©ì¤€
**ìµœì¢… ìˆ˜ì •**: 2025-12-30
**ë²„ì „**: 1.0
