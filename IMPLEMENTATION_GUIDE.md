# Reviewer: ë°•ìš©ì¤€ - í•µì‹¬ ìˆ˜ì •ì‚¬í•­ êµ¬í˜„ ê°€ì´ë“œ

**ì‘ì„±ì¼**: 2025-12-30
**ëª©ì **: debug_guide.md ë° ì¶”ê°€ ìš”êµ¬ì‚¬í•­ì— ë”°ë¥¸ ìµœì†Œ ì¹¨ìŠµ êµ¬í˜„ ê°€ì´ë“œ

---

## ì™„ë£Œëœ ì‘ì—…

### 1. âœ… A* Pathfinding ëª¨ë“ˆ êµ¬í˜„
- **íŒŒì¼**: `src/rl_dispatch/navigation/pathfinding.py`
- **ë‚´ìš©**:
  - `AStarPathfinder` í´ë˜ìŠ¤: 8ë°©í–¥ ì´ë™ A* êµ¬í˜„
  - `create_occupancy_grid_from_walls()`: ë²½ í´ë¦¬ê³¤ â†’ occupancy grid ë³€í™˜
  - `world_to_grid()`, `grid_to_world()`: ì¢Œí‘œ ë³€í™˜
  - `find_path()`, `get_distance()`, `path_exists()`: ê²½ë¡œ íƒìƒ‰ API

### 2. âœ… EnvConfig í™•ì¥
- **íŒŒì¼**: `src/rl_dispatch/core/config.py`
- **ì¶”ê°€ í•„ë“œ**:
  ```python
  grid_resolution: float = 0.5  # ê·¸ë¦¬ë“œ í•´ìƒë„
  walls: List[List[Tuple[float, float]]] = []  # ë²½ í´ë¦¬ê³¤ë“¤
  num_pedestrians: int = 0  # ë™ì  ì¥ì• ë¬¼ (ì‚¬ëŒ)
  num_vehicles: int = 0  # ë™ì  ì¥ì• ë¬¼ (ì°¨ëŸ‰/ì§€ê²Œì°¨)
  pedestrian_speed: float = 1.0
  vehicle_speed: float = 0.8
  dynamic_obstacle_radius: float = 0.5
  ```

### 3. âœ… ë§µ YAML ìŠ¤í‚¤ë§ˆ ì—…ë°ì´íŠ¸
- **íŒŒì¼**: `configs/map_large_square.yaml`
- **ì¶”ê°€ ë‚´ìš©**:
  ```yaml
  env:
    grid_resolution: 0.5
    walls:
      - [[40.0, 40.0], [60.0, 40.0], [60.0, 60.0], [40.0, 60.0]]  # ì¥ì• ë¬¼
      - [[20.0, 20.0], [25.0, 20.0], [25.0, 75.0], [20.0, 75.0]]  # Lì ë²½
  ```
  - **TODO**: ë‚˜ë¨¸ì§€ 5ê°œ ë§µì—ë„ ê°™ì€ ìŠ¤í‚¤ë§ˆ ì ìš©

---

## ğŸ”¥ ìš°ì„ ìˆœìœ„ ë†’ì€ ë¯¸ì™„ë£Œ ì‘ì—…

### 1. SimulatedNav2ì— A* í†µí•© (ì‹¬ê°ë„: ìµœìƒ)

**íŒŒì¼**: `src/rl_dispatch/navigation/nav2_interface.py`

**ìˆ˜ì • ë‚´ìš©**:
```python
# Reviewer: ë°•ìš©ì¤€
from rl_dispatch.navigation.pathfinding import AStarPathfinder

class SimulatedNav2(NavigationInterface):
    def __init__(
        self,
        occupancy_grid: np.ndarray,  # ì¶”ê°€
        grid_resolution: float = 0.5,  # ì¶”ê°€
        max_velocity: float = 1.5,
        nav_failure_rate: float = 0.05,
        collision_rate: float = 0.01,
        np_random: Optional[np.random.RandomState] = None,
    ):
        self.pathfinder = AStarPathfinder(occupancy_grid, grid_resolution)
        self.max_velocity = max_velocity
        # ...

    def get_eta(self, start: Tuple[float, float], goal: Tuple[float, float]) -> float:
        """A* ê¸°ë°˜ ETA ê³„ì‚°"""
        distance = self.pathfinder.get_distance(start, goal)
        if distance == np.inf:
            return np.inf  # ê²½ë¡œ ì—†ìŒ
        avg_velocity = self.max_velocity * 0.7
        return distance / avg_velocity

    def navigate_to_goal(self, start, goal) -> NavigationResult:
        """A* ê²½ë¡œ ê¸°ë°˜ ë‚´ë¹„ê²Œì´ì…˜"""
        result = self.pathfinder.find_path(start, goal)
        if result is None:
            return NavigationResult(time=0, success=False, collision=False)

        path, distance = result
        avg_velocity = self.max_velocity * 0.7
        nav_time = distance / avg_velocity * self.np_random.normal(1.0, 0.1)

        # ì‹¤íŒ¨ í™•ë¥ 
        success = self.np_random.random() > self.nav_failure_rate
        collision = self.np_random.random() < self.collision_rate if success else False

        return NavigationResult(
            time=nav_time,
            success=success and not collision,
            collision=collision,
            path=path if success else None
        )

    def plan_path(self, start, goal) -> Optional[List[Tuple[float, float]]]:
        """A* ê²½ë¡œ ê³„íš"""
        result = self.pathfinder.find_path(start, goal)
        return result[0] if result else None
```

**PatrolEnv ìˆ˜ì •** (`src/rl_dispatch/env/patrol_env.py`):
```python
from rl_dispatch.navigation.pathfinding import create_occupancy_grid_from_walls

class PatrolEnv:
    def __init__(self, env_config, reward_config):
        # Occupancy grid ìƒì„±
        self.occupancy_grid = create_occupancy_grid_from_walls(
            env_config.map_width,
            env_config.map_height,
            env_config.walls,
            env_config.grid_resolution
        )

        # Nav2 interfaceì— grid ì „ë‹¬
        self.nav_interface = SimulatedNav2(
            occupancy_grid=self.occupancy_grid,
            grid_resolution=env_config.grid_resolution,
            max_velocity=env_config.robot_max_velocity,
            np_random=self.np_random
        )
```

---

### 2. SMDP ê°€ë³€ í• ì¸ìœ¨ ì ìš© (ì‹¬ê°ë„: ìµœìƒ)

**íŒŒì¼**: `src/rl_dispatch/algorithms/buffer.py`

**ìˆ˜ì • ë‚´ìš©**:
```python
# Reviewer: ë°•ìš©ì¤€ - SMDP ê°€ë³€ í• ì¸ìœ¨
class RolloutBuffer:
    def __init__(self, buffer_size, obs_dim, gamma, gae_lambda, device):
        # ...
        self.nav_times = np.zeros(buffer_size, dtype=np.float32)  # ì¶”ê°€

    def add(self, obs, action, log_prob, reward, value, done, nav_time):  # nav_time ì¶”ê°€
        # ...
        self.nav_times[self.pos] = nav_time
        self.pos += 1

    def compute_returns_and_advantages(self, last_value, last_done=False):
        """GAE ê³„ì‚° with ê°€ë³€ í• ì¸ìœ¨"""
        last_gae_lam = 0
        for step in reversed(range(self.buffer_size)):
            # ...
            # ê°€ë³€ í• ì¸ìœ¨ ê³„ì‚° (dt_base = 1.0)
            nav_time = self.nav_times[step]
            gamma_k = self.gamma ** nav_time

            # TD error with gamma_k
            delta = (
                self.rewards[step] +
                gamma_k * next_value * next_non_terminal -
                self.values[step]
            )

            # GAE with gamma_k
            last_gae_lam = (
                delta +
                gamma_k * self.gae_lambda * next_non_terminal * last_gae_lam
            )
            self.advantages[step] = last_gae_lam

        self.returns = self.advantages + self.values
```

**í•™ìŠµ ë£¨í”„ ìˆ˜ì •** (`scripts/train_multi_map.py`):
```python
# Reviewer: ë°•ìš©ì¤€
# step í›„ nav_time ì €ì¥
next_obs, reward, terminated, truncated, info = env_wrapper.step(action)
nav_time = info.get("nav_time", 1.0)  # PatrolEnvì—ì„œ ì œê³µ

agent.buffer.add(
    obs=obs,
    action=action,
    reward=reward,
    value=value,
    log_prob=log_prob,
    done=terminated,
    nav_time=nav_time  # ì¶”ê°€
)
```

**PatrolEnv ìˆ˜ì •**:
```python
def step(self, action):
    # ...
    nav_result = self.nav_interface.navigate_to_goal(start, goal)
    nav_time = nav_result.time

    # ...
    info = {
        # ...
        "nav_time": nav_time  # ì¶”ê°€
    }
    return obs, reward, terminated, truncated, info
```

---

### 3. ì™„ì „í•œ í–‰ë™ ë§ˆìŠ¤í‚¹ (ì‹¬ê°ë„: ìƒ)

**íŒŒì¼**: `src/rl_dispatch/env/patrol_env.py`

**ìˆ˜ì • ë‚´ìš©**:
```python
# Reviewer: ë°•ìš©ì¤€ - í–‰ë™ ë§ˆìŠ¤í‚¹
def _compute_action_mask(self) -> np.ndarray:
    """í˜„ì¬ ìƒíƒœì—ì„œ ìœ íš¨í•œ í–‰ë™ ë§ˆìŠ¤í¬"""
    # mode_mask: [patrol ê°€ëŠ¥, dispatch ê°€ëŠ¥]
    mode_mask = np.ones(2, dtype=np.float32)

    # 1. ì´ë²¤íŠ¸ ì—†ìœ¼ë©´ dispatch ë¶ˆê°€
    if not self.current_state.has_event:
        mode_mask[1] = 0.0

    # 2. ë°°í„°ë¦¬ ë¶€ì¡±í•˜ë©´ dispatch ë¶ˆê°€
    if self.current_state.robot.battery_level < 0.2:
        mode_mask[1] = 0.0

    # 3. (ì„ íƒ) í›„ë³´ë³„ ë§ˆìŠ¤í¬ (ê²½ë¡œ ì—†ìŒ, keep-out zone ë“±)
    # replan_mask = np.ones(10, dtype=np.float32)
    # for i, candidate in enumerate(self.current_state.candidates):
    #     if not self.pathfinder.path_exists(robot_pos, candidate.next_goal):
    #         replan_mask[i] = 0.0

    return mode_mask

def _get_obs_and_info(self) -> Tuple[np.ndarray, Dict]:
    obs = self.obs_processor.process(self.current_state, update_stats=False)
    info = {
        "action_mask": self._compute_action_mask()  # ì¶”ê°€
    }
    return obs.vector, info

def reset(self, ...):
    # ...
    obs, info = self._get_obs_and_info()
    return obs, info

def step(self, action):
    # ...
    obs, info = self._get_obs_and_info()
    return obs, reward, terminated, truncated, info
```

**PPOAgent ìˆ˜ì •** (`src/rl_dispatch/algorithms/ppo.py`):
```python
# Reviewer: ë°•ìš©ì¤€
def update(self, last_value, last_done):
    # ...
    for batch in self.buffer.get(...):
        obs, actions, old_log_probs, advantages, returns, old_values, masks = batch

        # Forward with mask
        _, new_log_probs, entropy, values = self.network.get_action_and_value(
            obs, action=actions, mode_mask=masks
        )
        # ...
```

**RolloutBuffer ìˆ˜ì •**:
```python
def __init__(self, ...):
    # ...
    self.action_masks = np.zeros((buffer_size, 2), dtype=np.float32)  # ì¶”ê°€

def add(self, obs, action, log_prob, reward, value, done, nav_time, action_mask):
    # ...
    self.action_masks[self.pos] = action_mask  # ì¶”ê°€

def get(self, batch_size):
    # ...
    masks = torch.from_numpy(self.action_masks).to(self.device)
    yield (obs, actions, log_probs, advantages_t, returns, values, masks)  # masks ì¶”ê°€
```

---

### 4. ë°°í„°ë¦¬/ì¶©ì „ ë¡œì§ (ì‹¬ê°ë„: ìƒ)

**íŒŒì¼**: `src/rl_dispatch/env/patrol_env.py`

**ìˆ˜ì • ë‚´ìš©**:
```python
# Reviewer: ë°•ìš©ì¤€ - ë°°í„°ë¦¬ ê´€ë¦¬
class PatrolEnv:
    def step(self, action):
        # ë°°í„°ë¦¬ ì²´í¬
        if self.current_state.robot.battery_level < 0.15:
            # ê°•ì œë¡œ ì¶©ì „ì†Œë¡œ ì´ë™
            charging_pos = self.env_config.charging_station_position
            nav_result = self.nav_interface.navigate_to_goal(
                (self.current_state.robot.x, self.current_state.robot.y),
                charging_pos
            )

            # ì¶©ì „ì†Œ ë„ì°© â†’ ì¶©ì „
            if nav_result.success:
                distance_to_charging = np.sqrt(
                    (self.current_state.robot.x - charging_pos[0])**2 +
                    (self.current_state.robot.y - charging_pos[1])**2
                )
                if distance_to_charging < 2.0:  # 2m ë°˜ê²½
                    # ì¶©ì „ (50ì´ˆì— 100% ì¶©ì „)
                    charging_time = 50.0
                    self.current_state.robot.battery_level = 1.0
                    self.current_time += charging_time

                    info["charging"] = True
                    info["charging_time"] = charging_time

        # ë°°í„°ë¦¬ ì†Œëª¨ (ì´ë™ ì¤‘)
        battery_consumed = nav_result.time * self.env_config.robot_battery_drain_rate / 3600.0
        self.current_state.robot.battery_level = max(
            0.0,
            self.current_state.robot.battery_level - battery_consumed / self.env_config.robot_battery_capacity
        )

        # ...
```

---

### 5. ì´ë²¤íŠ¸ ìƒ˜í”Œë§ì„ Free-Spaceë¡œ ì œí•œ

**íŒŒì¼**: `src/rl_dispatch/env/patrol_env.py`

**ìˆ˜ì • ë‚´ìš©**:
```python
# Reviewer: ë°•ìš©ì¤€ - Free-space ì´ë²¤íŠ¸ ìƒì„±
def _maybe_generate_event(self, current_time, step_duration):
    # ... (ê¸°ì¡´ Poisson ìƒ˜í”Œë§)

    # Free-spaceì—ì„œ ìœ„ì¹˜ ìƒ˜í”Œë§ (ìµœëŒ€ 10íšŒ ì‹œë„)
    from rl_dispatch.navigation.pathfinding import AStarPathfinder

    for attempt in range(10):
        event_x = self.np_random.uniform(0, self.env_config.map_width)
        event_y = self.np_random.uniform(0, self.env_config.map_height)

        # Occupancy gridë¡œ free ì²´í¬
        grid_y, grid_x = self.pathfinder.world_to_grid(event_x, event_y)
        if self.occupancy_grid[grid_y, grid_x] == 0:  # Free
            # ë¡œë´‡ìœ¼ë¡œë¶€í„° ê²½ë¡œ ì¡´ì¬ í™•ì¸
            robot_pos = (self.current_state.robot.x, self.current_state.robot.y)
            if self.pathfinder.path_exists(robot_pos, (event_x, event_y)):
                # ì´ë²¤íŠ¸ ìƒì„±
                event = ExtendedEvent(...)
                return event

    # 10íšŒ ì‹œë„ ì‹¤íŒ¨ â†’ ì´ë²¤íŠ¸ ìƒì„± ì•ˆ í•¨
    return None
```

---

### 6. LiDAR Ray-casting êµ¬í˜„

**íŒŒì¼**: `src/rl_dispatch/env/patrol_env.py`

**ìˆ˜ì • ë‚´ìš©**:
```python
# Reviewer: ë°•ìš©ì¤€ - LiDAR ray-casting
def _simulate_lidar(self) -> np.ndarray:
    """Occupancy grid ê¸°ë°˜ ray-casting"""
    robot_pos = (self.current_state.robot.x, self.current_state.robot.y)
    robot_heading = self.current_state.robot.heading

    lidar_ranges = np.full(self.lidar_num_channels, self.lidar_max_range, dtype=np.float32)

    for i in range(self.lidar_num_channels):
        angle = robot_heading + (2 * np.pi * i / self.lidar_num_channels)

        # Ray-casting (Bresenham)
        for r in np.arange(self.lidar_min_range, self.lidar_max_range, self.env_config.grid_resolution):
            x = robot_pos[0] + r * np.cos(angle)
            y = robot_pos[1] + r * np.sin(angle)

            grid_y, grid_x = self.pathfinder.world_to_grid(x, y)

            # ë²”ìœ„ ì²´í¬
            if not (0 <= grid_y < self.occupancy_grid.shape[0] and
                    0 <= grid_x < self.occupancy_grid.shape[1]):
                lidar_ranges[i] = r
                break

            # ì¥ì• ë¬¼ ì¶©ëŒ
            if self.occupancy_grid[grid_y, grid_x] == 1:
                lidar_ranges[i] = r + self.np_random.normal(0, 0.02)  # ë…¸ì´ì¦ˆ
                break

    return lidar_ranges
```

---

### 7. ì €ìœ„í—˜ ì´ë²¤íŠ¸ ì²˜ë¦¬

**íŒŒì¼**: `src/rl_dispatch/env/patrol_env.py`

**ìˆ˜ì • ë‚´ìš©**:
```python
# Reviewer: ë°•ìš©ì¤€ - ì €ìœ„í—˜ ì´ë²¤íŠ¸ëŠ” ìˆœì°° ì¤‘ ê·¼ì ‘ í•´ê²°
def step(self, action):
    # ...

    # ì´ë²¤íŠ¸ê°€ ìˆê³ , risk_levelì´ ë‚®ìœ¼ë©´ (1-3) ìˆœì°° ì¤‘ ê·¼ì ‘ í™•ì¸
    if self.current_state.current_event and self.current_state.current_event.risk_level <= 3:
        event_pos = (self.current_state.current_event.x, self.current_state.current_event.y)
        robot_pos = (self.current_state.robot.x, self.current_state.robot.y)

        distance = np.sqrt(
            (event_pos[0] - robot_pos[0])**2 +
            (event_pos[1] - robot_pos[1])**2
        )

        # ë°˜ê²½ 5m ë‚´ ì§„ì… â†’ ìë™ í•´ê²°
        if distance < 5.0:
            self.current_state.current_event = None
            reward += self.reward_config.event_response_bonus * 0.5  # ì ˆë°˜ ë³´ìƒ
            info["low_risk_event_resolved"] = True

    # ê³ ìœ„í—˜ ì´ë²¤íŠ¸ (risk >= 7)ëŠ” ì¦‰ì‹œ dispatch í•„ìš”
    # ì¤‘ìœ„í—˜ (4-6)ì€ ì •ì±…ì´ íŒë‹¨
```

---

### 8. ìˆœì°° ì»¤ë²„ë¦¬ì§€ íŒ¨ë„í‹° ì¶”ê°€

**íŒŒì¼**: `src/rl_dispatch/rewards/reward_calculator.py`

**ìˆ˜ì • ë‚´ìš©**:
```python
# Reviewer: ë°•ìš©ì¤€ - ìˆœì°° ì»¤ë²„ë¦¬ì§€ íŒ¨ë„í‹°
def calculate_patrol_reward(self, state, next_state, action, config):
    # ê¸°ì¡´ visit bonus
    patrol_reward = 0.0
    if self._reached_patrol_point(...):
        patrol_reward += config.patrol_visit_bonus

    # âœ… ì¶”ê°€: ê³µë°± ë¹„ìš© (coverage gap penalty)
    gap_penalty = 0.0
    gap_threshold = 60.0  # 60ì´ˆ ì´ìƒ ë°©ë¬¸ ì•ˆ í•œ í¬ì¸íŠ¸

    for point in next_state.patrol_points:
        time_gap = next_state.current_time - point.last_visit_time
        if time_gap > gap_threshold:
            gap_penalty += config.patrol_gap_penalty_rate * (time_gap - gap_threshold)

    patrol_reward -= gap_penalty
    return patrol_reward
```

---

### 9. ë™ì  ì¥ì• ë¬¼ ì‹œë®¬ë ˆì´ì…˜ (ì‚¬ëŒ/ì¥ë¹„)

**ìƒˆ íŒŒì¼**: `src/rl_dispatch/env/dynamic_obstacles.py`

**ë‚´ìš©**:
```python
# Reviewer: ë°•ìš©ì¤€ - ë™ì  ì¥ì• ë¬¼
from dataclasses import dataclass
from typing import List, Tuple
import numpy as np

@dataclass
class DynamicObstacle:
    x: float
    y: float
    vx: float  # ì†ë„ (xë°©í–¥)
    vy: float  # ì†ë„ (yë°©í–¥)
    radius: float  # ì•ˆì „ ë°˜ê²½
    obstacle_type: str  # "pedestrian" or "vehicle"
    waypoints: List[Tuple[float, float]] = None  # ëª©í‘œ ì§€ì ë“¤

class DynamicObstacleManager:
    def __init__(self, num_pedestrians, num_vehicles, map_width, map_height, np_random):
        self.obstacles = []
        self.map_width = map_width
        self.map_height = map_height
        self.np_random = np_random

        # ì‚¬ëŒ ì´ˆê¸°í™” (ëœë¤ ì›Œí¬)
        for _ in range(num_pedestrians):
            x = np_random.uniform(5, map_width - 5)
            y = np_random.uniform(5, map_height - 5)
            self.obstacles.append(DynamicObstacle(
                x=x, y=y, vx=0, vy=0,
                radius=0.5, obstacle_type="pedestrian"
            ))

        # ì°¨ëŸ‰ ì´ˆê¸°í™” (waypoint ì™•ë³µ)
        for _ in range(num_vehicles):
            x = np_random.uniform(10, map_width - 10)
            y = np_random.uniform(10, map_height - 10)
            waypoints = [(x, y), (map_width - x, map_height - y)]  # ì™•ë³µ
            self.obstacles.append(DynamicObstacle(
                x=x, y=y, vx=0, vy=0,
                radius=1.0, obstacle_type="vehicle",
                waypoints=waypoints
            ))

    def update(self, dt: float, occupancy_grid: np.ndarray):
        """ë§¤ ìŠ¤í… ì¥ì• ë¬¼ ìœ„ì¹˜ ì—…ë°ì´íŠ¸"""
        for obs in self.obstacles:
            if obs.obstacle_type == "pedestrian":
                # ëœë¤ ì›Œí¬
                if self.np_random.random() < 0.3:  # 30% í™•ë¥ ë¡œ ë°©í–¥ ì „í™˜
                    angle = self.np_random.uniform(0, 2 * np.pi)
                    speed = 1.0  # m/s
                    obs.vx = speed * np.cos(angle)
                    obs.vy = speed * np.sin(angle)

                obs.x += obs.vx * dt
                obs.y += obs.vy * dt

                # ë§µ ê²½ê³„ ë°˜ì‚¬
                if obs.x < 2 or obs.x > self.map_width - 2:
                    obs.vx = -obs.vx
                if obs.y < 2 or obs.y > self.map_height - 2:
                    obs.vy = -obs.vy

            elif obs.obstacle_type == "vehicle":
                # Waypoint ê¸°ë°˜ ì´ë™ (ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì§ì„  ì´ë™)
                # TODO: A* ê²½ë¡œ ë”°ë¼ ì´ë™í•˜ë„ë¡ ê°œì„ 
                pass

    def get_dynamic_occupancy(self, grid_resolution: float) -> np.ndarray:
        """ë™ì  ì¥ì• ë¬¼ì˜ occupancy layer ìƒì„±"""
        grid_height = int(self.map_height / grid_resolution) + 1
        grid_width = int(self.map_width / grid_resolution) + 1
        dynamic_grid = np.zeros((grid_height, grid_width), dtype=np.uint8)

        for obs in self.obstacles:
            grid_x = int(obs.x / grid_resolution)
            grid_y = int(obs.y / grid_resolution)

            # ë°˜ê²½ ë‚´ ì…€ ì ìœ 
            r_cells = int(obs.radius / grid_resolution) + 1
            for dy in range(-r_cells, r_cells + 1):
                for dx in range(-r_cells, r_cells + 1):
                    gy, gx = grid_y + dy, grid_x + dx
                    if 0 <= gy < grid_height and 0 <= gx < grid_width:
                        dynamic_grid[gy, gx] = 1

        return dynamic_grid
```

**PatrolEnv í†µí•©**:
```python
from rl_dispatch.env.dynamic_obstacles import DynamicObstacleManager

class PatrolEnv:
    def __init__(self, ...):
        # ...
        self.dynamic_manager = DynamicObstacleManager(
            self.env_config.num_pedestrians,
            self.env_config.num_vehicles,
            self.env_config.map_width,
            self.env_config.map_height,
            self.np_random
        )

    def step(self, action):
        # 1. ë™ì  ì¥ì• ë¬¼ ì—…ë°ì´íŠ¸
        self.dynamic_manager.update(dt=nav_result.time, occupancy_grid=self.occupancy_grid)

        # 2. ì •ì  + ë™ì  occupancy ë³‘í•©
        dynamic_layer = self.dynamic_manager.get_dynamic_occupancy(self.env_config.grid_resolution)
        combined_grid = np.maximum(self.occupancy_grid, dynamic_layer)

        # 3. Nav2ëŠ” combined_grid ì‚¬ìš©
        self.nav_interface.pathfinder.grid = combined_grid  # ì—…ë°ì´íŠ¸

        # 4. ì¶©ëŒ ì²´í¬ (ë¡œë´‡ê³¼ ë™ì  ì¥ì• ë¬¼)
        for obs in self.dynamic_manager.obstacles:
            distance = np.sqrt(
                (self.current_state.robot.x - obs.x)**2 +
                (self.current_state.robot.y - obs.y)**2
            )
            if distance < obs.radius + 0.5:  # ë¡œë´‡ ë°˜ê²½ 0.5m
                reward += self.reward_config.collision_penalty
                info["dynamic_collision"] = True
```

---

### 10. ì‹œê°í™” with ë²½ ì˜¤ë²„ë ˆì´

**ìƒˆ íŒŒì¼**: `scripts/visualize_training_results.py`

**ë‚´ìš©**:
```python
#!/usr/bin/env python3
# Reviewer: ë°•ìš©ì¤€ - í•™ìŠµ ê²°ê³¼ ì‹œê°í™” (ë²½ ì˜¤ë²„ë ˆì´)
import sys
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import argparse

sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from rl_dispatch.core.config import EnvConfig
from rl_dispatch.env import create_multi_map_env

def visualize_coverage_with_walls(log_dir: Path, update_num: int = 400):
    """
    Coverage heatmap ìœ„ì— ë²½/ì¥ì• ë¬¼ ì˜¤ë²„ë ˆì´

    Args:
        log_dir: runs/multi_map_ppo/TIMESTAMP
        update_num: Update ë²ˆí˜¸ (ì˜ˆ: 400)
    """
    coverage_dir = log_dir / "coverage" / f"update_{update_num}"

    if not coverage_dir.exists():
        print(f"Coverage ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {coverage_dir}")
        return

    # 6ê°œ ë§µ ë¡œë“œ
    map_configs = [
        "configs/map_large_square.yaml",
        "configs/map_corridor.yaml",
        "configs/map_l_shaped.yaml",
        "configs/map_office_building.yaml",
        "configs/map_campus.yaml",
        "configs/map_warehouse.yaml",
    ]

    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.flatten()

    for idx, config_path in enumerate(map_configs):
        map_name = Path(config_path).stem
        heatmap_path = coverage_dir / f"{map_name}_heatmap.npy"

        if not heatmap_path.exists():
            print(f"Heatmap ì—†ìŒ: {heatmap_path}")
            continue

        # Heatmap ë¡œë“œ
        heatmap = np.load(heatmap_path)

        # Config ë¡œë“œ (ë²½ ì •ë³´)
        config = EnvConfig.load_yaml(config_path)

        ax = axes[idx]

        # Heatmap í‘œì‹œ
        im = ax.imshow(
            heatmap,
            cmap='hot',
            interpolation='bilinear',
            origin='lower',
            extent=[0, config.map_width, 0, config.map_height],
            alpha=0.7
        )

        # ë²½ ì˜¤ë²„ë ˆì´ (ì„ /ì»¨íˆ¬ì–´ë¡œ í‘œì‹œ)
        for wall in config.walls:
            if len(wall) < 2:
                continue

            # í´ë¦¬ê³¤ ê·¸ë¦¬ê¸°
            wall_array = np.array(wall)
            polygon = patches.Polygon(
                wall_array,
                closed=True,
                edgecolor='cyan',
                facecolor='none',
                linewidth=2,
                linestyle='-'
            )
            ax.add_patch(polygon)

        # ìˆœì°° í¬ì¸íŠ¸ í‘œì‹œ
        for i, (px, py) in enumerate(config.patrol_points):
            ax.plot(px, py, 'go', markersize=8, markeredgecolor='white', markeredgewidth=1)
            ax.text(px, py, f'P{i}', color='white', fontsize=8, ha='center', va='center')

        # ì¶©ì „ ìŠ¤í…Œì´ì…˜
        cx, cy = config.charging_station_position
        ax.plot(cx, cy, 'b^', markersize=12, markeredgecolor='white', markeredgewidth=1)
        ax.text(cx, cy + 3, 'Charging', color='white', fontsize=10, ha='center')

        ax.set_title(f"{map_name}\n(Update {update_num})", fontsize=12, fontweight='bold')
        ax.set_xlabel('X (m)')
        ax.set_ylabel('Y (m)')
        ax.grid(True, alpha=0.3)

        # Colorbar
        plt.colorbar(im, ax=ax, label='Visit Count')

    plt.tight_layout()

    # ì €ì¥
    output_dir = Path("outputs")
    output_dir.mkdir(exist_ok=True)
    output_path = output_dir / f"coverage_update_{update_num}.png"
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"âœ… Saved: {output_path}")
    plt.show()

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--log-dir", type=str, required=True, help="runs/multi_map_ppo/TIMESTAMP")
    parser.add_argument("--update", type=int, default=400, help="Update number")
    args = parser.parse_args()

    visualize_coverage_with_walls(Path(args.log_dir), args.update)
```

**ì‹¤í–‰ ëª…ë ¹ì–´**:
```bash
python scripts/visualize_training_results.py \
    --log-dir runs/multi_map_ppo/20251230-120000 \
    --update 400
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

ì™„ë£Œ í›„ ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:

- [ ] **ë²½ ê´€í†µ ì—†ìŒ**: ë¡œë´‡ì´ ë²½/ì¥ì• ë¬¼ì„ í†µê³¼í•˜ì§€ ì•Šê³  ìš°íšŒí•¨
- [ ] **ì´ë²¤íŠ¸ ë„ë‹¬ ê°€ëŠ¥**: ìƒì„±ëœ ì´ë²¤íŠ¸ê°€ ëª¨ë‘ free-spaceì´ë©° A* ê²½ë¡œê°€ ì¡´ì¬í•¨
- [ ] **ë°°í„°ë¦¬ ì¶©ì „ ë™ì‘**: ë°°í„°ë¦¬ lowì¼ ë•Œ ì¶©ì „ì†Œë¡œ ì´ë™í•˜ê³  ì¶©ì „í•¨
- [ ] **ì €ìœ„í—˜ ì´ë²¤íŠ¸**: risk_level ë‚®ì€ ì´ë²¤íŠ¸ëŠ” ì¦‰ì‹œ dispatchí•˜ì§€ ì•ŠìŒ (ìˆœì°° ì¤‘ ê·¼ì ‘ í•´ê²°)
- [ ] **í–‰ë™ ë§ˆìŠ¤í‚¹**: ì´ë²¤íŠ¸ ì—†ìŒ/ë°°í„°ë¦¬ ë¶€ì¡± ì‹œ dispatchê°€ ë§ˆìŠ¤í‚¹ë¨
- [ ] **SMDP í• ì¸ìœ¨**: nav_timeì— ë”°ë¼ gamma^(nav_time)ë¡œ í• ì¸ìœ¨ ì ìš©
- [ ] **ë™ì  ì¥ì• ë¬¼**: ì‚¬ëŒ/ì°¨ëŸ‰ì´ ì›€ì§ì´ë©°, ë¡œë´‡ì´ ì¶©ëŒí•˜ì§€ ì•Šê³  ìš°íšŒ/ëŒ€ê¸°í•¨
- [ ] **ì‹œê°í™”**: Coverage heatmapì— ë²½ì´ ì„ /ì»¨íˆ¬ì–´ë¡œ ì˜¤ë²„ë ˆì´ë¨

---

## ğŸš€ ì‹¤í–‰ ëª…ë ¹ì–´

### 1. í…ŒìŠ¤íŠ¸
```bash
# í™˜ê²½ í…ŒìŠ¤íŠ¸ (A*, ì´ë²¤íŠ¸ ìƒ˜í”Œë§, ë°°í„°ë¦¬)
python test_industrial_events.py
python test_nav2_and_heuristics.py

# Quick training (ìˆ˜ì • í›„ í…ŒìŠ¤íŠ¸)
python test_quick_training.py
```

### 2. í•™ìŠµ
```bash
# 100K steps (í…ŒìŠ¤íŠ¸ìš©)
python scripts/train_multi_map.py --total-timesteps 100000 --seed 42

# Full training (5M steps)
python scripts/train_multi_map.py --total-timesteps 5000000 --seed 42 --log-interval 10
```

### 3. ì‹œê°í™”
```bash
# Coverage heatmap with walls
python scripts/visualize_training_results.py \
    --log-dir runs/multi_map_ppo/<TIMESTAMP> \
    --update 400

# TensorBoard
tensorboard --logdir runs
```

---

## ğŸ“Œ ì°¸ê³ ì‚¬í•­

1. **ë‚˜ë¨¸ì§€ 5ê°œ ë§µ**: `configs/map_*.yaml` íŒŒì¼ë“¤ì—ë„ ê°™ì€ ë°©ì‹ìœ¼ë¡œ `walls` ì¶”ê°€ í•„ìš”
2. **ë™ì  ì¥ì• ë¬¼ ìˆ˜**: ì´ˆê¸°ì—ëŠ” `num_pedestrians=0, num_vehicles=0`ìœ¼ë¡œ ì‹œì‘, ì ì§„ì ìœ¼ë¡œ ì¦ê°€
3. **SMDP í• ì¸ìœ¨**: dt_base=1.0 (1ì´ˆ) ê¸°ì¤€, í•„ìš”ì‹œ ì¡°ì • ê°€ëŠ¥
4. **í…ŒìŠ¤íŠ¸ ìš°ì„ **: ê° ê¸°ëŠ¥ì„ ì¶”ê°€í•œ í›„ ë°˜ë“œì‹œ quick_testë¡œ ê²€ì¦

---

**ì‘ì„±ì**: Reviewer ë°•ìš©ì¤€
**ìµœì¢… ìˆ˜ì •**: 2025-12-30
