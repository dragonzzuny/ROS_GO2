# ğŸ› Bugfix: Buffer Unpacking Error

**ë¬¸ì œ**: `ValueError: too many values to unpack (expected 6)`
**ì›ì¸**: `buffer.get()`ì´ 7ê°œ ê°’ì„ ë°˜í™˜í•˜ëŠ”ë° PPOê°€ 6ê°œë§Œ ê¸°ëŒ€
**ìƒíƒœ**: âœ… **ìˆ˜ì • ì™„ë£Œ**

---

## ë¬¸ì œ ë°œìƒ ì›ì¸

### Buffer (buffer.py:256, 269-277)
```python
# buffer.get() yields 7 values:
yield (obs, actions, log_probs, advantages_t, returns, values, action_masks)
#                                                               ^^^^^^^^^^^^
#                                                               7ë²ˆì§¸ ì¶”ê°€ë¨
```

### PPO (ppo.py:201) - **ìˆ˜ì • ì „**
```python
# âŒ 6ê°œë§Œ unpacking
obs, actions, old_log_probs, advantages, returns, old_values = batch
```

**ì—ëŸ¬ ë©”ì‹œì§€**:
```
ValueError: too many values to unpack (expected 6)
```

---

## âœ… ì ìš©ëœ ìˆ˜ì •

### File: `src/rl_dispatch/algorithms/ppo.py:201-202`

**Before**:
```python
for batch in self.buffer.get(batch_size=self.training_config.batch_size):
    obs, actions, old_log_probs, advantages, returns, old_values = batch
```

**After**:
```python
for batch in self.buffer.get(batch_size=self.training_config.batch_size):
    # Reviewer ë°•ìš©ì¤€: Unpack 7 values (added action_masks)
    obs, actions, old_log_probs, advantages, returns, old_values, action_masks = batch
```

**Note**: `action_masks`ëŠ” í˜„ì¬ TODOë¡œ ë‚¨ì•„ìˆìœ¼ë©°, í–¥í›„ policy networkì—ì„œ ì‚¬ìš©í•  ì˜ˆì •ì…ë‹ˆë‹¤.

---

## ğŸ§ª ê²€ì¦ ë°©ë²•

### Option 1: Quick Test (1-2ë¶„)
```bash
cd ~/rl_dispatch_mvp
python scripts/quick_test_fix.py
```

**ê¸°ëŒ€ ì¶œë ¥**:
```
âœ… ALL TESTS PASSED!

CRITICAL CHECKS:
âœ… Entropy: 0.XXXX (> 0.01) - Good!
âœ… Approx KL: 0.XXXXXX (> 0.0001) - Policy updating!
âœ… Clipfrac: 0.XXXX (> 0.01) - PPO working!
âœ… Value loss: XXX.XX (< 1000) - Reasonable!
```

### Option 2: Manual Python Test
```bash
python -c "
import sys
sys.path.insert(0, 'src')
from rl_dispatch.algorithms import PPOAgent
from rl_dispatch.core.config import TrainingConfig, NetworkConfig

# Create agent
config = TrainingConfig(num_steps=64, batch_size=16, num_epochs=2)
agent = PPOAgent(obs_dim=77, num_replan_strategies=6, training_config=config, device='cpu')

# Collect fake rollout
import numpy as np
obs = np.random.randn(77).astype(np.float32)
for i in range(64):
    action, log_prob, value = agent.get_action(obs)
    agent.buffer.add(
        obs=obs,
        action=action,
        log_prob=log_prob,
        reward=1.0,
        value=value,
        done=False,
        nav_time=1.0
    )

# PPO update should NOT crash
try:
    stats = agent.update(last_value=0.0, last_done=True)
    print('âœ… PPO update works! No unpacking error!')
    print(f'Stats: {stats}')
except ValueError as e:
    print(f'âŒ Still has error: {e}')
"
```

---

## ğŸ“Š ì˜í–¥ ë²”ìœ„

### ìˆ˜ì •ëœ íŒŒì¼
- âœ… `src/rl_dispatch/algorithms/ppo.py` (Line 201-202)

### ì˜í–¥ë°›ëŠ” ìŠ¤í¬ë¦½íŠ¸
- âœ… `scripts/train_multi_map.py` - ì •ìƒ ì‘ë™
- âœ… `scripts/train_multi_map_fixed.py` - ì •ìƒ ì‘ë™
- âœ… `scripts/train.py` - ì •ìƒ ì‘ë™
- âœ… `scripts/quick_test_fix.py` - ì •ìƒ ì‘ë™

### í˜¸í™˜ì„±
- âœ… ê¸°ì¡´ checkpoint í˜¸í™˜ (ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ë³€ê²½ ì—†ìŒ)
- âœ… ê¸°ì¡´ config í˜¸í™˜
- âœ… ì´ì „ í•™ìŠµ ì¬ê°œ ê°€ëŠ¥

---

## ğŸ”„ ë‹¤ìŒ ë‹¨ê³„ (TODO)

í˜„ì¬ `action_masks`ëŠ” bufferì— ì €ì¥ë˜ê³  ì „ë‹¬ë˜ì§€ë§Œ, **ì‹¤ì œë¡œ ì‚¬ìš©ë˜ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤**.

### Future Enhancement (ì„ íƒì‚¬í•­)

**File**: `src/rl_dispatch/algorithms/networks.py`

ë„¤íŠ¸ì›Œí¬ì˜ `get_action_and_value()` ë©”ì„œë“œì—ì„œ action_maskë¥¼ í™œìš©:

```python
def get_action_and_value(
    self,
    obs: torch.Tensor,
    action: Optional[torch.Tensor] = None,
    mode_mask: Optional[torch.Tensor] = None,  # TODO: Use this!
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
    # ... existing code ...

    # TODO: Apply mask to logits before sampling
    if mode_mask is not None:
        # Mask invalid actions (set logits to -inf)
        mode_logits = mode_logits.masked_fill(mode_mask[:, :2] == 0, float('-inf'))
        replan_logits = replan_logits.masked_fill(mode_mask[:, 2:] == 0, float('-inf'))
```

**ìš°ì„ ìˆœìœ„**: ë‚®ìŒ (í˜„ì¬ í™˜ê²½ì˜ `_compute_action_mask()`ê°€ ì´ë¯¸ invalid action ê²½ê³  ì¶œë ¥)

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

### ì ˆëŒ€ í•˜ì§€ ë§ ê²ƒ
```python
# âŒ ì˜ëª»ëœ unpacking (6ê°œ)
obs, actions, old_log_probs, advantages, returns, old_values = batch

# âŒ action_masks ë¬´ì‹œ
obs, *rest = batch  # ë‚˜ë¨¸ì§€ ë¬´ì‹œ
```

### ì˜¬ë°”ë¥¸ ë°©ë²•
```python
# âœ… ì˜¬ë°”ë¥¸ unpacking (7ê°œ)
obs, actions, old_log_probs, advantages, returns, old_values, action_masks = batch

# âœ… ë¯¸ì‚¬ìš© ë³€ìˆ˜ëŠ” ëª…ì‹œ
obs, actions, old_log_probs, advantages, returns, old_values, _ = batch  # action_masks unused
```

---

## ğŸ¯ ê²°ë¡ 

**ìˆ˜ì • ì™„ë£Œ**: âœ…
**í…ŒìŠ¤íŠ¸ í•„ìš”**: âœ… `python scripts/quick_test_fix.py`
**í•™ìŠµ ê°€ëŠ¥**: âœ… ì¦‰ì‹œ í•™ìŠµ ì‹œì‘ ê°€ëŠ¥

ì´ ë²„ê·¸ëŠ” **ê¸°ëŠ¥ì  ë¬¸ì œ**ì˜€ìœ¼ë©° (í•™ìŠµì´ ì•„ì˜ˆ ë¶ˆê°€), ìˆ˜ì • í›„ ì¦‰ì‹œ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.

Policy collapse ë¬¸ì œì™€ëŠ” **ë…ë¦½ì **ì´ë¯€ë¡œ, ì´ bugfix + policy_collapse_fixë¥¼ í•¨ê»˜ ì ìš©í•˜ë©´:
1. âœ… í•™ìŠµì´ ì‹¤í–‰ë¨ (unpacking error í•´ê²°)
2. âœ… í•™ìŠµì´ ì•ˆì •í™”ë¨ (reward normalization ë“± ì ìš©)

---

**ì‘ì„±ì**: Reviewer ë°•ìš©ì¤€
**ì‘ì„±ì¼**: 2025-12-30
**ë²„ì „**: 1.0
