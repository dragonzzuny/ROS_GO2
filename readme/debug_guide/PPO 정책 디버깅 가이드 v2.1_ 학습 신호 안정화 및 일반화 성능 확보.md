# PPO 정책 디버깅 가이드 v2.1: 학습 신호 안정화 및 일반화 성능 확보

**문서 버전:** 2.1  
**검토 일자:** 2025년 12월 30일  
**작성자:** Manus AI (AI 수석 연구원)

---

## 1. Executive Summary

본 문서는 `rl_dispatch_mvp` 프로젝트의 학습 로그 재분석과 사용자 피드백을 통해, 기존 분석의 핵심 오해를 바로잡고 더 정확한 디버깅 가이드라인을 제공합니다. 

**기존 진단의 오해:**
- **보상 불균형:** 이벤트 보너스가 과도하다는 진단은 **사실과 반대**였습니다. 실제로는 누적되는 순찰 페널티(`reward_patrol`)가 너무 커서 미미한 이벤트 성공 보상(`reward_event`) 신호를 압살하는 구조입니다.
- **순찰 전략 부재:** 낮은 커버리지의 원인은 정책의 출동 편향성뿐만 아니라, ①대형 맵에서 본질적으로 낮은 커버리지 값, ②장기 미방문에 따른 페널티 폭주, ③Nav 실패가 순찰 실패로 간주되는 문제 등 **학습 불가능한 보상 신호 설계**에 있습니다.

**새로운 핵심 진단:**
1.  **Nav 즉시 실패 (Infeasible Goal):** 대형/복잡 맵(`campus`, `office_building`)에서 Return 값 폭락의 **1순위 원인은 목표 지점 생성 실패 및 Nav 즉시 실패**입니다. `nav_time < 1.0` 비율이 `campus`에서 95%에 달하는 것이 강력한 증거입니다.
2.  **학습 신호 왜곡:** 과도한 순찰 페널티 누적으로 인해 정책이 정상적인 탐색을 하지 못하고, 이벤트 대응과 같은 중요한 태스크의 학습 신호가 묻히고 있습니다.

따라서, 본 문서는 **(1) Nav 안정화 → (2) 보상 신호 재설계 → (3) 커리큘럼 도입**의 우선순위에 따라 문제 해결을 위한 구체적인 기술 가이드라인을 제시합니다.

## 2. 데이터 기반 근본 원인 재분석

### 2.1. (1순위 원인) Nav 즉시 실패 및 학습 환경 불안정성

`steps.csv` 로그 분석 결과, 대형 맵에서의 성능 붕괴는 보상 함수 이전에 **탐색 자체가 불가능한 상황**에 기인합니다.

| 맵 이름 | Nav 즉시 실패율 (`nav_time < 1.0`) | Nav 최종 실패율 (`nav_success=False`) |
|:---|:---:|:---:|
| `map_campus` | **95.29%** | **69.93%** |
| `map_office_building` | **78.11%** | **59.73%** |
| `map_warehouse` | 70.50% | 46.75% |
| `map_corridor` | 14.56% | 6.15% |
| `map_l_shaped` | 12.92% | 6.02% |
| `map_large_square` | 9.63% | 6.23% |

`campus` 맵에서 스텝의 95%가 1초 안에 끝난다는 것은, 정책이 목표를 정해도 `SimulatedNav2`가 **경로 계획 자체를 시작하지 못하고 즉시 실패(abort)를 반환**하고 있음을 의미합니다. 이는 주로 다음과 같은 원인으로 발생합니다.

-   **Infeasible Goal:** 후보 생성 단계에서 맵 구조를 고려하지 않아 벽 내부나 도달 불가능한 지점을 목표로 설정.
-   **A* Pathfinding 실패:** `SimulatedNav2` 내부의 경로 탐색 알고리즘이 복잡한 환경에서 경로를 찾는 데 실패.

이 문제가 해결되지 않으면, 정책은 아무리 학습해도 유의미한 행동을 할 수 없으며 Return 표준편차는 계속 폭발할 수밖에 없습니다. **커리큘럼 도입보다 이 문제 해결이 훨씬 시급합니다.**

### 2.2. (2순위 원인) 순찰 페널티 폭주로 인한 학습 신호 왜곡

사용자의 지적대로, 보상 불균형의 방향은 순찰 페널티가 너무 **미미한 것이 아니라, 너무 과도하여** 다른 신호를 덮어버리는 것입니다.

| 맵 이름 | 스텝당 평균 `reward_patrol` | 스텝당 평균 `reward_event` |
|:---|:---:|:---:|
| `map_campus` | -332.12 | +0.12 |
| `map_office_building` | -354.29 | +1.05 |
| `map_corridor` | -143.46 | -0.46 |

`campus` 맵에서 정책은 이벤트 성공으로 평균 `+0.12`의 보상을 받지만, 한 스텝마다 순찰 실패로 `-332`의 페널티를 받습니다. 이 상황에서는 이벤트 성공의 가치를 학습하기가 거의 불가능합니다. 이는 **보상 스케일링(Reward Scaling)**이 필요함을 명백히 보여줍니다.

## 3. 수정된 3단계 디버깅 가이드

### 3.1. 1단계 (최우선): Nav 안정화 및 Feasible 후보 생성

**목표:** `campus`, `office` 맵에서 Nav 즉시 실패율을 10% 미만으로 낮춥니다.

1.  **후보 유효성 검사 강화:** `CandidateGenerator`에서 후보 경로를 생성할 때, `SimulatedNav2.get_eta()`를 호출하여 ETA가 유효한(예: 무한대가 아닌) 경로만 `feasible=True`로 설정합니다. ETA 계산에 실제 A* 경로 탐색을 사용해야 합니다.

2.  **목표 지점 안전지대(Safe Zone) 설정:** 이벤트 또는 순찰 지점을 목표로 설정할 때, 해당 좌표가 `occupancy_grid` 상에서 장애물이 아닌지 확인하고, 만약 장애물이라면 가장 가까운 free space 지점으로 목표를 조정하는 로직을 추가합니다.

3.  **실패 시 재시도/대체 행동 로직:** Nav 실패가 즉시 에피소드 종료나 큰 페널티로 이어지지 않도록, "가만히 있기" 또는 "가장 가까운 유효 순찰 지점으로 이동"과 같은 간단한 대체 행동을 수행하게 하여 학습 안정성을 높입니다.

### 3.2. 2단계: 보상 함수 재설계 (학습 신호 정상화)

**목표:** 순찰 페널티의 과도한 누적을 막고, 이벤트 대응의 중요도를 현실적으로 반영합니다.

#### A. 순찰 보상: 누적 페널티에서 '상태 변화' 보상으로 변경

- **수정 제안:** `reward_calculator.py`에서 순찰 보상을 **ΔCoverage (커버리지 변화량)** 기반으로 변경합니다. 이는 스텝마다 페널티가 누적되는 것을 막고, 커버리지를 "개선하는" 행동에 직접적인 보상을 제공합니다.

```python
# src/rl_dispatch/rewards/reward_calculator.py

# calculate 함수 시그니처에 prev_coverage 추가
def calculate(self, ..., prev_coverage: float, current_coverage: float):
    # ...
    # _calculate_patrol_reward에 변화량 전달
    r_patrol = self._calculate_patrol_reward(prev_coverage, current_coverage)
    # ...

def _calculate_patrol_reward(self, prev_coverage: float, current_coverage: float) -> float:
    """커버리지 변화량에 기반한 보상을 계산합니다."""
    coverage_delta = current_coverage - prev_coverage
    
    # 커버리지가 증가하면 보상, 감소하면 페널티
    # w_patrol_delta: 50.0 ~ 100.0 (변화량이 작으므로 큰 가중치 필요)
    return self.config.w_patrol_delta * coverage_delta
```

#### B. 이벤트 보상: SLA 기반 스케일 재설정

- **수정 제안:** 이벤트 관련 보상/페널티를 임의의 값이 아닌, 현실적인 SLA(Service Level Agreement)에 기반하여 재설정합니다. 예를 들어, "이벤트 1회 실패 시 발생하는 비용(예: 1000달러)"을 기준으로 보상 스케일을 정합니다. 이는 이벤트 대응의 실제 가치를 정책이 학습하도록 합니다.

### 3.3. 3단계: 커리큘럼 학습 및 상태 공간 확장

Nav 안정화 및 보상 재설계가 완료된 후, 아래 개선안을 적용합니다.

1.  **커리큘럼 학습:** 이전 가이드에서 제안한 3-Phase 커리큘럼을 적용하여, 쉬운 맵에서 안정적인 Critic을 학습한 후 복잡한 맵으로 점진적으로 확장합니다.

2.  **상태 공간 확장 (의사결정 정보 강화):** `ObservationProcessor`를 수정하여 정책의 의사결정에 직접적으로 필요한 정보를 상태 벡터에 추가합니다. (77D → 80D+)
    -   **후보 ETA 및 유효성 (2D):** 선택된 후보의 예상 소요 시간(ETA)과 유효성(Feasible) 여부.
    -   **커버리지 요약 (1D+):** 전체 맵의 커버리지 비율 또는 미커버 지역의 중심 좌표 등.

## 4. 수정된 검증 체크포인트

1.  **Nav 즉시 실패율 (최우선):** `campus`, `office` 맵에서 `nav_time < 1.0` 비율이 **10% 미만**으로 급감하는가?
2.  **Return 표준편차 감소:** `campus` 맵의 `std_return`이 83k에서 **40k 이하로 유의미하게 감소**하는가?
3.  **순찰 커버리지 상승:** `campus`, `office` 맵에서 순찰 커버리지가 5%대에서 **15% 이상으로 상승**하는가? (ΔCoverage 보상 효과)
4.  **이벤트 성공률 유지/상승:** 커버리지가 상승하는 동안, 이벤트 성공률이 안정적으로 유지되거나 상승하는가?

---

*본 문서는 제출된 코드베이스, 학습 로그, 그리고 제공된 전문가 피드백을 종합하여 재작성되었습니다.*
