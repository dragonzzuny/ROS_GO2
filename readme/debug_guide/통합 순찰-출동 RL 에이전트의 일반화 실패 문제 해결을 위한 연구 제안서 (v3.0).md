> **"A good scientist is a person with original ideas. A good engineer is a person who makes a design that works with as few original ideas as possible."** - Freeman Dyson

# 통합 순찰-출동 RL 에이전트의 일반화 실패 문제 해결을 위한 연구 제안서 (v3.0)

**문서 버전:** 3.0 (논문 제안서 수준)
**검토 일자:** 2025년 12월 30일
**작성자:** Manus AI (AI 수석 연구원)

---

## 1. Abstract & Executive Summary

본 문서는 통합 순찰-출동 강화학습 프로젝트(`rl_dispatch_mvp`)의 현재 상태를 심층 분석하고, 관측된 **대형/복잡 맵에서의 일반화 성능 붕괴 현상**을 해결하기 위한 체계적인 연구 및 개발 로드맵을 제안한다. 초기 로그 분석에서 제기된 가설들(예: 보상 함수 불균형)은 추가적인 데이터 분석과 코드 검토를 통해 일부 오해를 포함하고 있었음이 밝혀졌다.

본 제안서의 **핵심 진단**은 다음과 같다:

1.  **환경 안정성 위기 (Feasibility Crisis):** 성능 붕괴의 1순위 근본 원인은 정책의 학습 실패가 아닌, **환경 자체의 불안정성**이다. `campus` 맵에서 95%에 달하는 행동이 1초 내에 실패(`nav_time < 1.0s`)하는 현상은, 에이전트가 도달 불가능한 목표(Infeasible Goal)를 생성하여 유의미한 탐색 기회 자체를 박탈당하고 있음을 명백히 보여준다.

2.  **학습 신호 대 잡음비 (SNR) 문제:** 현재 보상 함수는 두 가지 주요 왜곡을 포함한다. 첫째, `patrol_coverage_ratio`의 정의가 에피소드 길이에 종속되어 있어, 맵 크기나 에피소드 길이에 따라 페널티가 폭주하며 다른 학습 신호를 압살한다. 둘째, `campus` 맵과 같이 이벤트가 극도로 희소한(1.94%) 환경에서는 이벤트 대응 학습이 거의 불가능하다.

3.  **행동 공간의 한계:** 현재 `ActionMode`는 `PATROL`과 `DISPATCH`만으로 구성되어 있으며, 배터리 소모가 거의 없어(`min: 95.7%`) 충전, 대기 등 현실적인 장기 운영에 필요한 행동이 원천적으로 배제되어 있다.

따라서 본 문서는 단순한 파라미터 튜닝을 넘어, **(1) 환경 안정성 확보 → (2) 보상 및 상태 표현력 강화 → (3) 학습 전략 고도화**의 3단계 접근법을 통해 문제를 근본적으로 해결하고, 최종적으로 논문 수준의 강건한 연구 결과를 도출하기 위한 구체적인 실험 설계와 기술적 구현 방안을 제시한다.

---

## 2. 문제 정의 및 데이터 기반 재분석

### 2.1. 제1 원인: 환경의 구조적 불안정성 (Infeasible Goal Catastrophe)

가장 시급하고 심각한 문제는 대형 맵에서 발생하는 Nav 즉시 실패 현상이다. 이는 정책의 문제가 아닌, 환경 설계의 결함이다.

| 맵 이름 | Nav 즉시 실패율 (`nav_time < 1.0s`) | Nav 최종 실패율 (`nav_success=False`) |
|:---|:---:|:---:|
| `map_campus` | **95.29%** | 69.93% |
| `map_office_building` | 78.11% | 59.73% |
| `map_warehouse` | 70.50% | 46.75% |
| `map_corridor` | 14.56% | 6.15% |

`campus` 맵에서 95%의 행동이 1초 내에 실패한다는 것은, 에이전트가 학습할 기회조차 얻지 못하고 무의미한 행동을 반복하며 막대한 페널티를 받고 있음을 의미한다. 이는 Return 값의 폭발적인 분산으로 직접 이어진다. **이 문제를 해결하지 않고 보상 함수나 커리큘럼을 논하는 것은 무의미하다.**

### 2.2. 제2 원인: 왜곡된 보상 신호와 부적절한 KPI

#### 2.2.1. 순찰 보상의 스케일링 실패

사용자의 지적대로, 순찰 페널티는 미미한 것이 아니라 오히려 너무 커서 다른 신호를 압도한다.

- **문제:** `patrol_coverage_ratio`는 `visit_interval = episode_time / num_points`에 기반하여, 에피소드가 길어질수록 모든 지점이 "정상"으로 간주되는 비직관적인 결과를 낳는다. 반면, `reward_patrol` 항은 스텝마다 누적되어 에피소드 길이에 따라 무한히 발산하며, `campus` 맵에서 스텝당 평균 -332의 압도적인 페널티를 생성한다.
- **결론:** 현재 순찰 보상은 학습 가능한 신호가 아닌, 에피소드 길이에 종속된 **잡음(Noise)**에 가깝다.

#### 2.2.2. 현실과 동떨어진 KPI

- **문제:** `patrol_coverage_ratio` 5~15%는 정책의 실패라기보다, 대형 맵에서 해당 KPI가 구조적으로 낮게 나올 수밖에 없음을 시사한다. Return 값 역시 Nav 실패 페널티와 순찰 페널티 누적으로 인해 왜곡되어, 실제 임무 성공도를 제대로 반영하지 못한다.
- **결론:** `Return`과 `Coverage %`만으로는 정책의 실제 성능을 오판할 수 있다. 논문 수준의 주장을 위해서는 운영 관점의 현실적인 KPI가 필수적이다.

### 2.3. 제3 원인: 제한된 행동/상태 공간과 희소한 이벤트

- **행동 공간:** 코드 분석 결과, `ActionMode`는 `PATROL`과 `DISPATCH` 2가지로 명확히 제한된다. `WAIT`, `CHARGE` 등의 행동은 현재 구현되어 있지 않다. 배터리 소모가 거의 없어 충전의 필요성 또한 학습 데이터에 나타나지 않는다.
- **상태 공간:** 현재 상태 벡터는 "왜 지금 순찰 대신 출동해야 하는가?"라는 trade-off를 판단할 핵심 정보(예: 이벤트의 긴급성, 가장 오래 방치된 구역의 위험도)를 충분히 담고 있지 않다.
- **이벤트 희소성:** `campus` 맵에서 이벤트 발생 스텝은 단 1.94%에 불과하다. 이는 이벤트 대응 전략을 학습하기에 극도로 비효율적인 환경이다.

---

## 3. 제안 연구 로드맵 (Methodology Proposal)

아래 3단계 로드맵은 문제의 우선순위에 따라 체계적으로 실험을 진행할 것을 제안한다.

### 3.1. 1단계 (Foundation): 환경 안정성 및 신뢰성 확보

**가설:** Nav 즉시 실패율을 낮추면, 대형 맵에서의 Return 분산이 유의미하게 감소할 것이다.

**실험 1.1: Feasible Waypoint Pool 도입**
- **구현:** 학습 시작 전, 각 맵에 대해 A* 알고리즘으로 상호 도달 가능한 `Waypoint`들의 집합을 미리 계산한다. 모든 순찰 지점과 이벤트 발생 위치는 이 Pool 내에서만 샘플링되도록 강제한다.
- **측정:** `campus`, `office` 맵에서 `nav_time < 1.0s` 비율, `nav_success` 비율.
- **성공 기준:** Nav 즉시 실패율이 10% 미만으로 감소.

**실험 1.2: 행동 공간 확장 및 안전 장치 추가**
- **구현:**
    1.  `ActionMode`에 `WAIT`(제자리 대기)를 추가한다. 이는 Nav 실패 시 페널티를 받고 에피소드를 끝내는 대신, 잠시 대기하며 상황을 다시 판단할 기회를 제공한다.
    2.  배터리 소모 시나리오를 강화하고(예: 이동 시 더 많은 배터리 소모), `GO_CHARGE` 행동과 충전소 위치를 환경에 추가한다. 배터리가 특정 임계치(예: 30%) 이하일 때만 `GO_CHARGE`가 유효하도록 마스킹한다.
- **측정:** 에피소드 당 `WAIT` 행동 비율, 배터리 임계치 도달 빈도, `GO_CHARGE` 선택 빈도.
- **성공 기준:** Nav 실패 시 에피소드가 즉시 종료되지 않고, `WAIT` 행동을 통해 학습이 지속됨. 배터리 제약 하에서 충전 행동을 학습하기 시작함.

### 3.2. 2단계 (Representation): 보상 및 상태 표현력 강화

**가설:** 학습 가능한 형태의 보상과 상태를 제공하면, 정책이 순찰과 출동의 trade-off를 효과적으로 학습할 것이다.

**실험 2.1: 보상 함수 재설계 (Reward Shaping & Normalization)**
- **구현:**
    1.  **순찰 보상:** 기존 `patrol_coverage_ratio`를 폐기하고, **"시간당 신규 커버리지 면적(ΔArea/sec)"** 또는 **"가장 오래 방치된 지점의 공백 시간(Max Gap Time) 감소량"**으로 변경한다. 이는 스케일이 제어되고, 에이전트의 행동과 직접적으로 연관된 보상 신호를 제공한다.
    2.  **이벤트 보상:** SLA 기반으로 재설계한다. (예: `event_success = +1000`, `event_fail = -5000`, `latency_penalty = -10/sec`)
    3.  **보상 정규화:** 각 보상 구성요소를 `RunningMeanStd` 필터로 정규화하여, 특정 보상 항이 Gradient를 독점하는 현상을 방지한다.
- **측정:** 정규화된 각 보상 항의 분산, 정책 엔트로피, 최종 KPI (아래 3.4절 참조).
- **성공 기준:** 모든 보상 항이 0에 가깝지 않은 안정적인 분산을 보이며, 정책 엔트로피가 급격히 감소하지 않음.

**실험 2.2: 상태 공간 확장 (Decision-Critical Information)**
- **구현:** `ObservationProcessor`를 수정하여 다음 정보를 상태 벡터에 추가한다.
    -   **이벤트 정보 (3D):** `(urgency * confidence, time_to_deadline, is_present)`
    -   **순찰 위기 정보 (3D):** `(max_gap_time, overdue_points_count, nearest_overdue_point_vector)`
    -   **후보 정보 (N x 3D):** 각 후보 `k`에 대해 `(expected_eta, expected_battery_cost, is_feasible)`
    -   **배터리 정보 (2D):** `(battery_level, time_to_charge_station)`
- **측정:** Ablation Study를 통해 각 정보 그룹의 기여도(V-function 예측 정확도, 최종 성능)를 평가한다.
- **성공 기준:** 확장된 상태 공간을 사용한 정책이 기본 상태 공간 정책 대비 최종 KPI에서 우월한 성능을 보임.

### 3.3. 3단계 (Strategy): 학습 전략 고도화

**가설:** 환경과 보상이 안정화된 후, 커리큘럼 학습과 희소 이벤트 샘플링을 통해 학습 효율을 극대화할 수 있다.

**실험 3.1: 맵 난이도 기반 커리큘럼 학습**
- **구현:** 이전 가이드의 3-Phase 커리큘럼을 적용하되, 각 단계의 전환 조건을 `episodes`가 아닌 **"이전 단계 맵들에서 KPI 안정화 여부"**로 설정한다.
- **측정:** Critic의 Value 예측 손실(MSE), 맵별 KPI 수렴 속도.
- **성공 기준:** 커리큘럼을 적용했을 때, 최종 성능에 더 빨리 도달하거나 더 높은 점근선(asymptote)을 달성함.

**실험 3.2: 이벤트 희소성 극복 (Prioritized Experience Replay 변형)**
- **구현:** 학습 데이터 수집 시, 이벤트가 발생한 Trajectory에 더 높은 가중치를 부여하여 PPO 업데이트 시 더 자주 샘플링되도록 한다. (단, 평가 시에는 실제 이벤트 발생률을 사용)
- **측정:** 동일 학습 시간 대비 이벤트 대응 성공률.
- **성공 기준:** 이벤트가 희소한 `campus` 맵에서도 이벤트 대응 능력을 효과적으로 학습함.

---

## 4. 제안: 새로운 핵심 성과 지표 (KPIs for Publication)

`Return`과 `Coverage %`를 대체/보완할 아래의 현실적인 KPI를 도입하여 정책의 다면적 성능을 평가할 것을 제안한다.

| KPI Category | Metric | Definition | Rationale |
|:---|:---|:---|:---|
| **임무 효율성** | **평균 응답 시간 (Mean Response Time)** | 이벤트 발생부터 로봇 도착까지의 평균 시간 | 출동 정책의 핵심 성능 척도 |
| | **SLA 위반율 (SLA Violation Rate)** | 마감 시간 내에 처리되지 못한 이벤트의 비율 | 서비스 품질을 직접적으로 측정 |
| **순찰 품질** | **최대 순찰 공백 (Max Patrol Gap)** | 모든 순찰 지점 중 가장 오래 방문되지 않은 시간 | 순찰 네트워크의 가장 취약한 지점을 평가 |
| | **위험 가중 커버리지 (Risk-Weighted Coverage)** | 중요도(위험도)가 높은 구역의 방문 빈도 | 모든 지역을 동일하게 보는 대신, 핵심 구역 방어 능력을 평가 |
| **운영 안정성** | **Nav 실패율 (Navigation Failure Rate)** | 전체 이동 시도 중 Nav 실패(abort, timeout) 비율 | 에이전트의 기본적인 이동 신뢰성 척도 |
| | **에너지 효율성 (Energy Efficiency)** | 임무 당 평균 이동 거리 또는 배터리 소모량 | 장기 운영 비용과 직결되는 지표 |

이 KPI들은 정책의 장단점을 명확히 보여주며, 논문에서 연구의 기여를 주장하기 위한 강력한 근거를 제공할 것이다.

---

## 5. 결론 및 다음 단계

본 제안서는 현재 프로젝트가 직면한 문제가 정책 학습의 한계가 아닌, **환경의 근본적인 불안정성과 왜곡된 학습 신호**에 있음을 명확히 했다. 따라서 제안된 3단계 로드맵에 따라 **환경 안정화부터 순차적으로 진행**하는 것이 가장 효율적이고 성공 확률이 높은 접근법이다.

**즉시 실행할 다음 단계:**
1.  **실험 1.1 (Feasible Waypoint Pool)을 구현하고, `campus` 맵에서 Nav 즉시 실패율이 10% 미만으로 떨어지는지 확인한다.**

이 첫 단추가 성공적으로 끼워져야만, 이후의 보상 및 상태 공간 연구가 의미를 가질 수 있다. 이 과정을 통해 얻어지는 결과는 통합 순찰-출동 RL 연구 분야에서 중요한 기여를 할 수 있는 견고한 기반이 될 것이다. 
